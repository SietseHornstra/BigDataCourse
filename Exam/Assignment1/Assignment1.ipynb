{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLUQpgdAyJnq"
   },
   "source": [
    "## General instructions\n",
    "\n",
    "Please fill out the answers to the questions below in markdown blocks (for questions) and coding blocks (for coding exercises). \n",
    "\n",
    "For some programming questions, some hints have already been provided for you. Add additional blocks if you need them (e.g. to explain your answers). Try to answer each question succinctly. \n",
    "\n",
    "In the coding exercises, you may use ``pandas``, ``numpy`` and ``scipy`` routines, but **not** scikit-learn (``sklearn``). You may use ``matplotlib`` or ``seaborn`` for plotting.\n",
    "\n",
    "Submit the completed notebook after filling in all the questions and please make sure that the answers are visible without needing to execute each code block (i.e. so the code block has already been executed). When marking the assignment we will not run any code if this has not been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI statement\n",
    "\n",
    "Please state if you used AI tools in preparing your answers, and if so please explain what you used them for. Note that it is not acceptable to directly paste answers or code generated by AI tools to solve these problems. In cases of doubt we may plan an interview to discuss verbally your understanding of the solutions to the exercises before releasing the final grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Algebra and Data Handling\n",
    "\n",
    "### Task 1.1 (1 point)\n",
    "\n",
    "Suppose that you are given a data matrix (X) that summarises the expenditure of 10 different hospitals across a 6 month period, where the the hospitals are stored one per row and the months are stored one per column. Print out a vector that you can multiply this matrix with to yield the following quantities. In other words, give the vector v that causes the matrix-vector product **X * v** to yield the following:\n",
    " \n",
    "1. The difference in the total expenditure for each hospital between the first three and the last three months\n",
    "2. The total expenditure of the first two months minus the average expenditure for each hospital for the the last four monthmonths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://numpy.org/doc/2.2/reference/routines.linalg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nC0Wd_jWyJnr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86220534, 0.85626928, 0.05969501, 0.44111732, 0.47585788,\n",
       "       0.32301471, 0.48965985, 0.96147361, 0.32379384, 0.86785134])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(10,6) # dummy data\n",
    "\n",
    "\n",
    "#Difference in total expenditure for each hospital between the first three and last three months. \n",
    "v1 = np.array([1, 1, 1, -1, -1, -1])\n",
    "\n",
    "X@v1\n",
    "\n",
    "v2 = np.array([1,1,-0.25,-0.25,-0.25,-0.25])\n",
    "\n",
    "X@v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwqxZOEByJns"
   },
   "source": [
    "### Task 1.2 (1 point)\n",
    "\n",
    "Write a short piece of code that uses an eigendecomposition to determine the rank of the following matrix. Check your answer by computing the rank directly using the function np.linalg.matrix_rank():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://numpy.org/doc/2.2/reference/routines.linalg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kaa4fBnsyJns"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[ -2., 20., -6.,  7.,  7., 4.],\n",
    "              [ 9., 5., 9.,  7.,  7., 0.],\n",
    "              [ -1., 3., 4.,  -2.,  9., -27.],\n",
    "              [ 8., 11., 2.,  9.,  4., 16.],\n",
    "              [ 4., 19.,  -3.,  8.,  8., 7.],\n",
    "              [ 6., 7.,  7., 10.,  4., 11.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VvlxipudyJns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 202.  208.  135.  215.  128.  241.]\n",
      " [ 208.  965.  -49.  490.  426.  385.]\n",
      " [ 135.  -49.  195.   77.   69.  -44.]\n",
      " [ 215.  490.   77.  347.  220.  392.]\n",
      " [ 128.  426.   69.  220.  275.  -51.]\n",
      " [ 241.  385.  -44.  392.  -51. 1171.]]\n",
      "[1.89916684e+03+0.00000000e+00j 8.87844522e+02+0.00000000e+00j\n",
      " 3.54086352e+02+0.00000000e+00j 1.39022859e+01+0.00000000e+00j\n",
      " 9.25517269e-15+7.94399643e-15j 9.25517269e-15-7.94399643e-15j]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sym = X.T @ X\n",
    "print(sym)\n",
    "\n",
    "eigenvalues = np.linalg.eigvals(sym)\n",
    "print(eigenvalues)\n",
    "\n",
    "##TODO\n",
    "#change \n",
    "tol = 1e-10\n",
    "rank_from_eig = np.sum(eigenvalues > tol)\n",
    "\n",
    "\n",
    "print(rank_from_eig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "rank_direct = np.linalg.matrix_rank(X)\n",
    "print(rank_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 (2 points)\n",
    "\n",
    "Download the data below and perform a basic quality control procedure using basic statistical operations. You can assume that the data contain demographic measures plus some biological features (it does not matter what these are for the purposes of this exercies).\n",
    "\n",
    "Then answer the following  questions: \n",
    "\n",
    "1. Which subject has the most missing data? How many missing features does this subject have?\n",
    "2. Which are the features most likely to contain outliers? How many samples would you remove? Give reasons for your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-06 11:45:24--  https://raw.githubusercontent.com/predictive-clinical-neuroscience/BigDataCourse/main/data/qc_dataset.csv\n",
      "Herleiden van raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Verbinding maken met raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... verbonden.\n",
      "HTTP-verzoek is verzonden; wachten op antwoord... 200 OK\n",
      "Lengte: 648561 (633K) [text/plain]\n",
      "Wordt opgeslagen als: ‘qc_dataset.csv’\n",
      "\n",
      "qc_dataset.csv      100%[===================>] 633,36K  --.-KB/s    in 0,03s   \n",
      "\n",
      "2025-12-06 11:45:24 (19,3 MB/s) - '‘qc_dataset.csv’' opgeslagen [648561/648561]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget -nc https://raw.githubusercontent.com/predictive-clinical-neuroscience/BigDataCourse/main/data/qc_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 501 entries, 0 to 500\n",
      "Data columns (total 73 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      501 non-null    int64  \n",
      " 1   age     501 non-null    int64  \n",
      " 2   sex     501 non-null    int64  \n",
      " 3   f0      501 non-null    float64\n",
      " 4   f1      501 non-null    float64\n",
      " 5   f2      501 non-null    float64\n",
      " 6   f3      501 non-null    float64\n",
      " 7   f4      501 non-null    float64\n",
      " 8   f5      501 non-null    float64\n",
      " 9   f6      501 non-null    float64\n",
      " 10  f7      501 non-null    float64\n",
      " 11  f8      501 non-null    float64\n",
      " 12  f9      501 non-null    float64\n",
      " 13  f10     501 non-null    float64\n",
      " 14  f11     501 non-null    float64\n",
      " 15  f12     501 non-null    float64\n",
      " 16  f13     501 non-null    float64\n",
      " 17  f14     501 non-null    float64\n",
      " 18  f15     501 non-null    float64\n",
      " 19  f16     501 non-null    float64\n",
      " 20  f17     501 non-null    float64\n",
      " 21  f18     501 non-null    float64\n",
      " 22  f19     501 non-null    float64\n",
      " 23  f20     500 non-null    float64\n",
      " 24  f21     501 non-null    float64\n",
      " 25  f22     501 non-null    float64\n",
      " 26  f23     501 non-null    float64\n",
      " 27  f24     501 non-null    float64\n",
      " 28  f25     501 non-null    float64\n",
      " 29  f26     501 non-null    float64\n",
      " 30  f27     501 non-null    float64\n",
      " 31  f28     501 non-null    float64\n",
      " 32  f29     501 non-null    float64\n",
      " 33  f30     501 non-null    float64\n",
      " 34  f31     501 non-null    float64\n",
      " 35  f32     501 non-null    float64\n",
      " 36  f33     501 non-null    float64\n",
      " 37  f34     501 non-null    float64\n",
      " 38  f35     501 non-null    float64\n",
      " 39  f36     501 non-null    float64\n",
      " 40  f37     501 non-null    float64\n",
      " 41  f38     501 non-null    float64\n",
      " 42  f39     501 non-null    float64\n",
      " 43  f40     501 non-null    float64\n",
      " 44  f41     501 non-null    float64\n",
      " 45  f42     501 non-null    float64\n",
      " 46  f43     501 non-null    float64\n",
      " 47  f44     500 non-null    float64\n",
      " 48  f45     501 non-null    float64\n",
      " 49  f46     501 non-null    float64\n",
      " 50  f47     501 non-null    float64\n",
      " 51  f48     501 non-null    float64\n",
      " 52  f49     501 non-null    float64\n",
      " 53  f50     501 non-null    float64\n",
      " 54  f51     501 non-null    float64\n",
      " 55  f52     501 non-null    float64\n",
      " 56  f53     501 non-null    float64\n",
      " 57  f54     501 non-null    float64\n",
      " 58  f55     500 non-null    float64\n",
      " 59  f56     501 non-null    float64\n",
      " 60  f57     501 non-null    float64\n",
      " 61  f58     501 non-null    float64\n",
      " 62  f59     501 non-null    float64\n",
      " 63  f60     501 non-null    float64\n",
      " 64  f61     501 non-null    float64\n",
      " 65  f62     501 non-null    float64\n",
      " 66  f63     501 non-null    float64\n",
      " 67  f64     501 non-null    float64\n",
      " 68  f65     500 non-null    float64\n",
      " 69  f66     501 non-null    float64\n",
      " 70  f67     501 non-null    float64\n",
      " 71  f68     501 non-null    float64\n",
      " 72  f69     501 non-null    float64\n",
      "dtypes: float64(70), int64(3)\n",
      "memory usage: 285.9 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "qc_dataset = pd.read_csv('/Users/sietsehornstra/Desktop/GitHub/BigDataCourse/Exam/Assignment1/qc_dataset.csv')\n",
    "\n",
    "qc_dataset.head()\n",
    "qc_dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>n_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>217</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  n_missing\n",
       "217  217          2\n",
       "56    56          1\n",
       "369  369          1\n",
       "331  331          0\n",
       "332  332          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 \n",
    "qc_dataset['n_missing'] = qc_dataset.iloc[:, 1:].isna().sum(axis=1)\n",
    "qc_dataset[['id','n_missing']].sort_values('n_missing', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 (1 point)\n",
    "\n",
    "What do we use the Digital Research Environment for?\n",
    "\n",
    "1. Data collection\n",
    "2. Data processing and analysis\n",
    "3. Collaborating on shared data with external parties\n",
    "4. Archiving\n",
    "5. Making data FAIR\n",
    "\n",
    "select all answers that apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIatJnjQyJns"
   },
   "source": [
    "# Part 2: Machine learning and statistics\n",
    "\n",
    "### Task 2.1 (1 point)\n",
    "\n",
    "1. Describe the function of the parameter $\\gamma$ in support vector machines (following the notation given in the lectures). How does it prevent overfitting?\n",
    "2. Linear models have different kinds of parameters. Briefly describe the different functions of the parameters denoted $w$ and $\\theta$ given in the machine learning lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muZv-1VsyJns"
   },
   "source": [
    "### Task 2.2 (1 point)\n",
    "\n",
    "Considering the following scenario: \n",
    "\n",
    "A researcher wants to train a classifier to predict Parkinson's disease and acquires some neuroimaging data to do this. First, the researcher selects the most informative features from the imaging data using a t-test. Next, the researcher trains a classifier on these features using cross-validation and obtains classification accuracy of 75% for discriminating patients from controls. The researcher shows that this is statistically better than chance (50%) using a binomial test. \n",
    "\n",
    "Would you consider this to be a solid workflow? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNy2-Yl6yJnt"
   },
   "source": [
    "### Task 2.3 (1 point)\n",
    "\n",
    "Suppose that you are the data scientist working for clinic that performs biopsies for lung cancer. In this clinic, you know that using a 'gold standard' test, approximately 5% of the samples that are tested come back positive for lung cancer and you test approximately 1000 people per year. You are evaluating a new test that according to the manufacturer has a sensitivity of approximately 80% and a specificity of 95% but is appealing because it is much faster than the gold standard test, meaning that the patients will have their results the same day instead of having to wait a week.\n",
    "\n",
    "First, write a small block of python code to estimate the accuracy, positive and negative predictive value of the test under the scenario above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KEE2avqyJnt"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgSUSAPwyJnt"
   },
   "source": [
    "### Task 2.4 (1 point)\n",
    "\n",
    "Would you switch to the new test? Give reasons for your answer. Can you think of factors that would change your preference? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_M7J4tqyJnt"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpAfhPyVyJnt"
   },
   "source": [
    "### Task 2.5 (1 point)\n",
    "\n",
    "Consider the following scenario: a researcher would like to use a clustering algorithm to find subtypes of asthma. The researcher acquires biomedical data, including: 3 lung function parameters from a breath outflow test, 10 blood-based markers (that you can assume are known to be associated with asthma) and 100 genetic variables (that have each been associated with asthma in at least one study). The researcher trains a K-means algorithm across 2-5 clusters and finds that the 5 cluster solution is the most reproducible (statistically significant at p < 0.01). \n",
    "\n",
    "Please answer the following questions: \n",
    "\n",
    "1. Would you consider this to be acceptable evidence to determine that biological subtypes exist? \n",
    "2. Please briefly outline what addtional validation steps you would recommend to determine whether the clusters can be used to improve prediction of outcomes in asthma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFdbYHzAyJn5"
   },
   "source": [
    "### Task 2.6 (1 point)\n",
    "\n",
    "Matrix decomposition techniques are important ways to reduce dimensionality in big data cohorts. Provide brief answers to the following questions: \n",
    "\n",
    "1. What are the steps needed to perform principal components analysis (PCA) on the basis of an eigendecomposition?\n",
    "2. What is the difference between running linked ICA on multimodal data from concatenating the data and running PCA?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6KSz0ouyJn5"
   },
   "source": [
    "## Part 3: Analysis of Parkinson's disease dataset\n",
    "\n",
    "For this part of the assignment, we will work with electronic measurements of voice characteristics from 42 people with early-stage Parkinson's disease. These participants were included in a six-month trial of a telemonitoring device for remote symptom progression monitoring. The motivation is that Parkinson's disease affects the characteristics of the voice in a way that might be associated with disease progression. See [here](https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring) for a description of the data. Note that the UPDRS (Unified Parkinson's Disease Rating Scale) is a standard scale for rating the symptoms of Parkinson's disease across different domains.\n",
    "\n",
    "For this assignment, we have split the dataset into two parts, which you can download here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY2quKQnyJn5"
   },
   "outputs": [],
   "source": [
    "#!wget -nc https://raw.githubusercontent.com/predictive-clinical-neuroscience/BigDataCourse/main/data/parkinsons_updrs_dataset1.csv\n",
    "#!wget -nc https://raw.githubusercontent.com/predictive-clinical-neuroscience/BigDataCourse/main/data/parkinsons_updrs_dataset2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9dt77USyJn5"
   },
   "source": [
    "### Task 3.0 (1 point, bonus question)\n",
    "Load the data and count the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WPTDxQ_yJn5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lcIMnWiyJn5"
   },
   "source": [
    "### Task 3.1 (2 points)\n",
    "\n",
    "Your first task is to perform PCA on the first data matrix (\"parkinsons_updrs_dataset1\"), then:\n",
    "\n",
    "* plot the eigenvalues sorted from largest to smallest\n",
    "* print the number of components you would need to retain 99.9% of the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bge-LtvyJn5"
   },
   "source": [
    "### Task 3.2 (1 point)\n",
    "\n",
    "Your next task is to fit a GLM to predict symptom severity ('total_UPDRS') on the basis of age, sex and the 16 biomedical voice measurements using only the first part of the Parkinson dataset. Don't forget to account for the fact that the symptom severity does not have a zero mean. Print out the regression coefficients and make a plot of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_G6z79XyJn5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmXw4EEryJn5"
   },
   "source": [
    "### Task 3.3 (1 point)\n",
    "\n",
    "Now, evaluate how accurately this model can predict the true symptom scores. To do this compute the correlation between the true and predicted symptom scores as well as the explained variance score. Print these values. \n",
    "\n",
    "Hint: the explained variance can be computed as $1-var(y-\\hat{y})/var(y)$ where $y$ and $\\hat{y}$ are the true and predicted labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thmZRw6tyJn5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTGhZxvdyJn5"
   },
   "source": [
    "### Task 3.4 (1 point)\n",
    "\n",
    "Now compute the predictions on the second dataset using the coefficients estimated on the first dataset. Compute and print the correlation and explained variance as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILZJhEdKyJn5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-YtJY6OyJn5"
   },
   "source": [
    "### Task 3.5 (1 point)\n",
    "\n",
    "Now, we are going to interpret these results. Please answer the following questions:\n",
    "\n",
    "1. Can you see evidence for overfitting? why or why not?  \n",
    "2. Which do you think might be the most important explanatory variables ? Explain why you think that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhsjbg2xyJn5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hTu6r88yJn5"
   },
   "source": [
    "### Task 3.6 (2 points)\n",
    "\n",
    "Now write a piece of code to do PCA on the set of features mentioned in task 3.2 above (i.e. age, sex and the 16 biomedical voice measurements) from the first dataset. \n",
    "\n",
    "Then predict the total UPDRS score for the second dataset on the basis of the first 3 principal components. In order to make sure this is unbiased do this in a way that ensures the second dataset is completely independent (i.e. as if someone else has it). \n",
    "\n",
    "Compute the explained variance and compare with what you have above. In your opinion does using three principal components provide adequate compression for this task? Why or why not?\n",
    "\n",
    "_Hint: you will need to project the data onto the principal components. This can be done efficiently using a matrix operation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
